{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luigiselmi/timeseries/blob/main/traffic_forecast_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df2a345-cb56-412b-b8e6-d0a40fefc61e",
      "metadata": {
        "id": "3df2a345-cb56-412b-b8e6-d0a40fefc61e"
      },
      "source": [
        "# Metro Interstate Traffic Volume Forecasting (Part 2)\n",
        "This notebook is based on chapter 13 of the book [*Time Series Forecasting in Python*](https://www.manning.com/books/time-series-forecasting-in-python-book) by Marco Peixeiro. Classical algorithms for time series forecasting, are\n",
        "\n",
        "* $Moving Average MA(q)$\n",
        "* $Auto Regressive AR(p)$\n",
        "* $Autoregressive Moving Average ARMA(p,q)$\n",
        "* $Autoregressive Integrated Moving Average ARIMA(p,d,q)$\n",
        "* $SARIMA(p,d,q)(P,D,Q)_m$\n",
        "* $SARIMAX$\n",
        "\n",
        "where q is the order, or lag, of the time series, that is the number of previous elements in the series that are used for the forecast, p is the autocorrelation order, that is number of elements that are correlated. These models try to fit a set of basis functions to the data depending on the chosen order $(p,d,q)(P,D,Q)_m$. With the neural networks we fit an unknown nonlinear function to the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a71a77ee-9bab-4365-98c0-9d900ff8aae0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a71a77ee-9bab-4365-98c0-9d900ff8aae0",
        "outputId": "01b68712-35c8-4137-bc4b-6bcb132db0f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Pandas version: 2.2.2\n",
            "Matplotlib version: 3.10.0\n",
            "TensorFlow versiob: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "print('NumPy version: %s'%np.__version__)\n",
        "print('Pandas version: %s'%pd.__version__)\n",
        "print('Matplotlib version: %s'%mpl.__version__)\n",
        "print('TensorFlow versiob: %s'%tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data windowing\n",
        "In order to train a model we split the data into time windows of the same length, for instance 24 hours. If the task is multistep forecasting, we use e.g. 24 hours as input data and the next 24 hours as target values so that a data window contains 48 hours. For example, the 1st window uses 48 hours starting from the 1st hour, the 2nd window uses 48 hours starting from the 2nd hour and so on. So there is an overlap of 47 hours between two consecutive data windows. We can create batches using a certain number of data windows, e.g. 32 data windows. The order for each data window cannot be changed but the order of the data windows in a batch can be shuffled."
      ],
      "metadata": {
        "id": "bIyfu8Nm-8py"
      },
      "id": "bIyfu8Nm-8py"
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('../data/train.csv', index_col=0)\n",
        "val_df = pd.read_csv('../data/val.csv', index_col=0)\n",
        "test_df = pd.read_csv('../data/test.csv', index_col=0)\n",
        "\n",
        "print(train_df.shape, val_df.shape, test_df.shape)"
      ],
      "metadata": {
        "id": "7qY3RHi-IB3N",
        "outputId": "c6d18f02-e57c-44ca-fcd3-664600fd245e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "id": "7qY3RHi-IB3N",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-37214b33b866>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/val.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataWindow():\n",
        "    def __init__(self, input_width, label_width, shift,\n",
        "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "                 label_columns=None):\n",
        "\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.test_df = test_df\n",
        "\n",
        "        self.label_columns = label_columns\n",
        "        if label_columns is not None:\n",
        "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
        "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
        "\n",
        "        self.input_width = input_width\n",
        "        self.label_width = label_width\n",
        "        self.shift = shift\n",
        "\n",
        "        self.total_window_size = input_width + shift\n",
        "\n",
        "        self.input_slice = slice(0, input_width)\n",
        "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "        self.label_start = self.total_window_size - self.label_width\n",
        "        self.labels_slice = slice(self.label_start, None)\n",
        "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "    def split_to_inputs_labels(self, features):\n",
        "        inputs = features[:, self.input_slice, :]\n",
        "        labels = features[:, self.labels_slice, :]\n",
        "        if self.label_columns is not None:\n",
        "            labels = tf.stack(\n",
        "                [labels[:,:,self.column_indices[name]] for name in self.label_columns],\n",
        "                axis=-1\n",
        "            )\n",
        "        inputs.set_shape([None, self.input_width, None])\n",
        "        labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def plot(self, model=None, plot_col='traffic_volume', max_subplots=3):\n",
        "        inputs, labels = self.sample_batch\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plot_col_index = self.column_indices[plot_col]\n",
        "        max_n = min(max_subplots, len(inputs))\n",
        "\n",
        "        for n in range(max_n):\n",
        "            plt.subplot(3, 1, n+1)\n",
        "            plt.ylabel(f'{plot_col} [scaled]')\n",
        "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
        "                     label='Inputs', marker='.', zorder=-10)\n",
        "\n",
        "            if self.label_columns:\n",
        "              label_col_index = self.label_columns_indices.get(plot_col, None)\n",
        "            else:\n",
        "              label_col_index = plot_col_index\n",
        "\n",
        "            if label_col_index is None:\n",
        "              continue\n",
        "\n",
        "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
        "                        edgecolors='k', marker='s', label='Labels', c='green', s=64)\n",
        "            if model is not None:\n",
        "              predictions = model(inputs)\n",
        "              plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
        "                          marker='X', edgecolors='k', label='Predictions',\n",
        "                          c='red', s=64)\n",
        "\n",
        "            if n == 0:\n",
        "              plt.legend()\n",
        "\n",
        "        plt.xlabel('Time (h)')\n",
        "\n",
        "    def make_dataset(self, data):\n",
        "        data = np.array(data, dtype=np.float32)\n",
        "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "            data=data,\n",
        "            targets=None,\n",
        "            sequence_length=self.total_window_size,\n",
        "            sequence_stride=1,\n",
        "            shuffle=True,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        ds = ds.map(self.split_to_inputs_labels)\n",
        "        return ds\n",
        "\n",
        "    @property\n",
        "    def train(self):\n",
        "        return self.make_dataset(self.train_df)\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self.make_dataset(self.val_df)\n",
        "\n",
        "    @property\n",
        "    def test(self):\n",
        "        return self.make_dataset(self.test_df)\n",
        "\n",
        "    @property\n",
        "    def sample_batch(self):\n",
        "        result = getattr(self, '_sample_batch', None)\n",
        "        if result is None:\n",
        "            result = next(iter(self.train))\n",
        "            self._sample_batch = result\n",
        "        return result"
      ],
      "metadata": {
        "id": "hcnvt7YXHv55",
        "outputId": "8f672c07-7a1b-45df-a6bc-10fec86ad5a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "id": "hcnvt7YXHv55",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e186c0fd8d1d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDataWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, input_width, label_width, shift, \n\u001b[1;32m      3\u001b[0m                  \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  label_columns=None):\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e186c0fd8d1d>\u001b[0m in \u001b[0;36mDataWindow\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDataWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     def __init__(self, input_width, label_width, shift, \n\u001b[0;32m----> 3\u001b[0;31m                  \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                  label_columns=None):\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4818cf-9279-46ba-8683-a7739f0b1482",
      "metadata": {
        "id": "0b4818cf-9279-46ba-8683-a7739f0b1482"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}